{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMN+BeDs/srPpXuW9rDcdTX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uTvGWQONmqn3","executionInfo":{"status":"ok","timestamp":1716252258552,"user_tz":360,"elapsed":21684,"user":{"displayName":"model_training","userId":"15059446775298759120"}},"outputId":"015b6384-7bf6-4352-e278-cf91f4ad2308"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["corpus=\"/content/drive/MyDrive/JCRSextoSemestre/GeneracionEtiquetado/salida.txt\""],"metadata":{"id":"tpSgHR71mmIa","executionInfo":{"status":"ok","timestamp":1716252296035,"user_tz":360,"elapsed":172,"user":{"displayName":"model_training","userId":"15059446775298759120"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"v7labnPyitj9","executionInfo":{"status":"ok","timestamp":1716252306879,"user_tz":360,"elapsed":6112,"user":{"displayName":"model_training","userId":"15059446775298759120"}}},"outputs":[],"source":["import numpy as np\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.preprocessing import LabelEncoder\n","from keras.utils import to_categorical\n","\n","# Función para leer las oraciones etiquetadas desde un archivo de texto\n","def read_tagged_sentences(file_path):\n","    sentences = []\n","    tags = []\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        sentence = []\n","        tag_seq = []\n","        for line in file:\n","            line = line.strip()\n","            if line:\n","                word, tag = line.split('#')\n","                sentence.append(word)\n","                tag_seq.append(tag)\n","            else:\n","                if sentence:\n","                    sentences.append(sentence)\n","                    tags.append(tag_seq)\n","                    sentence = []\n","                    tag_seq = []\n","        if sentence:\n","            sentences.append(sentence)\n","            tags.append(tag_seq)\n","    return sentences, tags\n","\n","# Leer las oraciones etiquetadas desde los archivos de entrenamiento y prueba\n","train_sentences, train_tags = read_tagged_sentences(corpus)\n","test_sentences, test_tags = read_tagged_sentences(corpus)\n"]},{"cell_type":"code","source":["# Tokenizar las palabras\n","word_tokenizer = Tokenizer()\n","word_tokenizer.fit_on_texts(train_sentences)\n","\n","# Convertir palabras a secuencias de enteros\n","X_train = word_tokenizer.texts_to_sequences(train_sentences)\n","X_test = word_tokenizer.texts_to_sequences(test_sentences)\n","\n","# Padding para asegurar que todas las secuencias tengan la misma longitud\n","max_len = max(len(s) for s in train_sentences)\n","X_train = pad_sequences(X_train, padding='post', maxlen=max_len)\n","X_test = pad_sequences(X_test, padding='post', maxlen=max_len)\n","\n","# Tokenizar las etiquetas\n","tag_tokenizer = Tokenizer()\n","tag_tokenizer.fit_on_texts(train_tags)\n","\n","# Convertir etiquetas a secuencias de enteros\n","y_train = tag_tokenizer.texts_to_sequences(train_tags)\n","y_test = tag_tokenizer.texts_to_sequences(test_tags)\n","\n","# Padding para las etiquetas\n","y_train = pad_sequences(y_train, padding='post', maxlen=max_len)\n","y_test = pad_sequences(y_test, padding='post', maxlen=max_len)\n","\n","# Convertir etiquetas a categorías\n","num_tags = len(tag_tokenizer.word_index) + 1\n","y_train = [to_categorical(i, num_classes=num_tags) for i in y_train]\n","y_test = [to_categorical(i, num_classes=num_tags) for i in y_test]\n","y_train = np.array(y_train)\n","y_test = np.array(y_test)\n"],"metadata":{"id":"BnOh7ddEl60o","executionInfo":{"status":"ok","timestamp":1716252416810,"user_tz":360,"elapsed":182,"user":{"displayName":"model_training","userId":"15059446775298759120"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Embedding, LSTM, Dense, TimeDistributed, Dropout, Bidirectional\n","\n","# Parámetros del modelo\n","embedding_dim = 128\n","lstm_units = 64\n","\n","# Construcción del modelo\n","model = Sequential()\n","model.add(Embedding(input_dim=len(word_tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_len))\n","model.add(Bidirectional(LSTM(units=lstm_units, return_sequences=True, recurrent_dropout=0.1)))\n","model.add(TimeDistributed(Dense(units=num_tags, activation='softmax')))\n","\n","# Compilación del modelo\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Resumen del modelo\n","model.summary()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sm9inNtSmF0a","executionInfo":{"status":"ok","timestamp":1716252424195,"user_tz":360,"elapsed":860,"user":{"displayName":"model_training","userId":"15059446775298759120"}},"outputId":"1640a71b-9665-4403-9cf5-6f90d94f37d7"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 324, 128)          8448      \n","                                                                 \n"," bidirectional (Bidirection  (None, 324, 128)          98816     \n"," al)                                                             \n","                                                                 \n"," time_distributed (TimeDist  (None, 324, 13)           1677      \n"," ributed)                                                        \n","                                                                 \n","=================================================================\n","Total params: 108941 (425.55 KB)\n","Trainable params: 108941 (425.55 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# Entrenamiento del modelo\n","history = model.fit(X_train, y_train, batch_size=32, epochs=10, validation_split=0.2, verbose=1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321},"id":"AdumzWQfmKPZ","executionInfo":{"status":"error","timestamp":1716252517992,"user_tz":360,"elapsed":194,"user":{"displayName":"model_training","userId":"15059446775298759120"}},"outputId":"ba7b55ff-cec5-4ee0-9e8f-6a3ba3a86b74"},"execution_count":7,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Training data contains 1 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.2`. Either provide more data, or a different value for the `validation_split` argument.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-eb4e22597b7f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Entrenamiento del modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36mtrain_validation_split\u001b[0;34m(arrays, validation_split)\u001b[0m\n\u001b[1;32m   1793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msplit_at\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msplit_at\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1795\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1796\u001b[0m             \u001b[0;34m\"Training data contains {batch_dim} samples, which is not \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1797\u001b[0m             \u001b[0;34m\"sufficient to split it into a validation and training set as \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Training data contains 1 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.2`. Either provide more data, or a different value for the `validation_split` argument."]}]},{"cell_type":"code","source":["# Evaluación del modelo\n","evaluation = model.evaluate(X_test, y_test, verbose=1)\n","print(f'Model Accuracy: {evaluation[1] * 100:.2f}%')\n"],"metadata":{"id":"hAgSC0Q8mb2R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generar predicciones\n","y_pred = model.predict(X_test, verbose=1)\n","y_pred = np.argmax(y_pred, axis=-1)\n","\n","# Convertir etiquetas de enteros a etiquetas originales\n","inv_tag_index = {v: k for k, v in tag_tokenizer.word_index.items()}\n","y_test_labels = [[inv_tag_index[np.argmax(tag)] for tag in seq] for seq in y_test]\n","y_pred_labels = [[inv_tag_index[tag] for tag in seq] for seq in y_pred]\n","\n","# Evaluar el rendimiento\n","from sklearn.metrics import classification_report\n","\n","# Aplanar las listas de etiquetas para evaluación\n","y_test_flat = [label for seq in y_test_labels for label in seq]\n","y_pred_flat = [label for seq in y_pred_labels for label in seq]\n","\n","# Imprimir el informe de clasificación\n","print(classification_report(y_test_flat, y_pred_flat))\n"],"metadata":{"id":"lsr38DVpmjbf"},"execution_count":null,"outputs":[]}]}