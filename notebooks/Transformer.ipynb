{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyObED95MYpobG2LaFI2dQYs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BYJIgI5hFeWv","executionInfo":{"status":"ok","timestamp":1716519747239,"user_tz":360,"elapsed":21418,"user":{"displayName":"model_training","userId":"15059446775298759120"}},"outputId":"bb903cac-6002-4fe5-8afa-68835c71a053"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["train=\"/content/drive/MyDrive/JCRSextoSemestre/GeneracionEtiquetado/train.txt\"\n","test=\"/content/drive/MyDrive/JCRSextoSemestre/GeneracionEtiquetado/test.txt\""],"metadata":{"id":"MZLa_5MZFZwv","executionInfo":{"status":"ok","timestamp":1716519754003,"user_tz":360,"elapsed":253,"user":{"displayName":"model_training","userId":"15059446775298759120"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","\n","# Función para leer las oraciones etiquetadas desde un archivo de texto\n","def read_tagged_sentences(file_path):\n","    sentences = []\n","    tags = []\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        sentence = []\n","        tag_seq = []\n","        for line in file:\n","            line = line.strip()\n","            if line:\n","                word, tag = line.split('#')\n","                sentence.append(word)\n","                tag_seq.append(tag)\n","            else:\n","                if sentence:\n","                    sentences.append(sentence)\n","                    tags.append(tag_seq)\n","                    sentence = []\n","                    tag_seq = []\n","        if sentence:\n","            sentences.append(sentence)\n","            tags.append(tag_seq)\n","    return sentences, tags\n","\n","# Leer las oraciones etiquetadas desde los archivos de entrenamiento y prueba\n","train_sentences, train_tags = read_tagged_sentences(train)\n","test_sentences, test_tags = read_tagged_sentences(test)\n","\n","# Tokenizar las palabras y etiquetas\n","word_tokenizer = Tokenizer()\n","word_tokenizer.fit_on_texts(train_sentences)\n","\n","tag_tokenizer = Tokenizer()\n","tag_tokenizer.fit_on_texts(train_tags)\n","\n","# Convertir palabras y etiquetas a secuencias de enteros\n","X_train = word_tokenizer.texts_to_sequences(train_sentences)\n","X_test = word_tokenizer.texts_to_sequences(test_sentences)\n","\n","y_train = tag_tokenizer.texts_to_sequences(train_tags)\n","y_test = tag_tokenizer.texts_to_sequences(test_tags)\n","\n","# Padding de las secuencias\n","max_len = max(len(seq) for seq in X_train)\n","X_train = pad_sequences(X_train, padding='post', maxlen=max_len)\n","X_test = pad_sequences(X_test, padding='post', maxlen=max_len)\n","\n","y_train = pad_sequences(y_train, padding='post', maxlen=max_len)\n","y_test = pad_sequences(y_test, padding='post', maxlen=max_len)\n","\n","# Convertir etiquetas a formato categórico\n","num_tags = len(tag_tokenizer.word_index) + 1\n","y_train = [to_categorical(i, num_classes=num_tags) for i in y_train]\n","y_test = [to_categorical(i, num_classes=num_tags) for i in y_test]\n","y_train = np.array(y_train)\n","y_test = np.array(y_test)\n"],"metadata":{"id":"rx_e9wN3DQf9","executionInfo":{"status":"ok","timestamp":1716520024889,"user_tz":360,"elapsed":739,"user":{"displayName":"model_training","userId":"15059446775298759120"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":790},"id":"JTLPY1RPCkiU","executionInfo":{"status":"error","timestamp":1716520041441,"user_tz":360,"elapsed":2169,"user":{"displayName":"model_training","userId":"15059446775298759120"}},"outputId":"89b9011a-5ad6-4950-fa75-58dbb957887c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 8854)]            0         \n","                                                                 \n"," token_and_position_embeddi  (None, 8854, 64)          603456    \n"," ng (TokenAndPositionEmbedd                                      \n"," ing)                                                            \n","                                                                 \n"," transformer_block (Transfo  (None, 8854, 64)          41792     \n"," rmerBlock)                                                      \n","                                                                 \n"," dense_2 (Dense)             (None, 8854, 64)          4160      \n","                                                                 \n"," dropout_2 (Dropout)         (None, 8854, 64)          0         \n","                                                                 \n"," dense_3 (Dense)             (None, 8854, 135)         8775      \n","                                                                 \n","=================================================================\n","Total params: 658183 (2.51 MB)\n","Trainable params: 658183 (2.51 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]},{"output_type":"error","ename":"ValueError","evalue":"Training data contains 1 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.2`. Either provide more data, or a different value for the `validation_split` argument.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-b818c705be29>\u001b[0m in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Entrenar el modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# Evaluar el modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36mtrain_validation_split\u001b[0;34m(arrays, validation_split)\u001b[0m\n\u001b[1;32m   1793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msplit_at\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msplit_at\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1795\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1796\u001b[0m             \u001b[0;34m\"Training data contains {batch_dim} samples, which is not \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1797\u001b[0m             \u001b[0;34m\"sufficient to split it into a validation and training set as \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Training data contains 1 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.2`. Either provide more data, or a different value for the `validation_split` argument."]}],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, Embedding, Dropout, LayerNormalization, MultiHeadAttention, Add\n","from tensorflow.keras.optimizers import Adam\n","\n","class TransformerBlock(tf.keras.layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super(TransformerBlock, self).__init__()\n","        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = tf.keras.Sequential(\n","            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim),]\n","        )\n","        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = Dropout(rate)\n","        self.dropout2 = Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)\n","\n","class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n","    def __init__(self, maxlen, vocab_size, embed_dim):\n","        super(TokenAndPositionEmbedding, self).__init__()\n","        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n","\n","    def call(self, x):\n","        maxlen = tf.shape(x)[-1]\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","        x = self.token_emb(x)\n","        return x + positions\n","\n","embed_dim = 64  # Dimensión de las embeddings\n","num_heads = 2  # Número de cabezas de atención\n","ff_dim = 64  # Dimensión de la red feed-forward interna\n","maxlen = X_train.shape[1]  # Longitud máxima de las secuencias\n","vocab_size = len(word_tokenizer.word_index) + 1  # Tamaño del vocabulario\n","\n","inputs = Input(shape=(maxlen,))\n","embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n","x = embedding_layer(inputs)\n","transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n","x = transformer_block(x)\n","x = Dense(ff_dim, activation=\"relu\")(x)\n","x = Dropout(0.1)(x)\n","outputs = Dense(num_tags, activation=\"softmax\")(x)\n","\n","model = Model(inputs=inputs, outputs=outputs)\n","\n","model.compile(optimizer=Adam(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n","model.summary()\n","\n","# Entrenar el modelo\n","history = model.fit(X_train, y_train, batch_size=32, epochs=5, validation_split=0.2, verbose=1)\n","\n","# Evaluar el modelo\n","score = model.evaluate(X_test, y_test, verbose=1)\n","print(f\"Test accuracy: {score[1]}\")\n"]},{"cell_type":"code","source":["# Guardar el modelo y los tokenizadores\n","model.save('pos_tagger_transformer_model.h5')\n","\n","import pickle\n","\n","with open('word_tokenizer.pkl', 'wb') as file:\n","    pickle.dump(word_tokenizer, file)\n","\n","with open('tag_tokenizer.pkl', 'wb') as file:\n","    pickle.dump(tag_tokenizer, file)"],"metadata":{"id":"NJfmi63XENfO"},"execution_count":null,"outputs":[]}]}